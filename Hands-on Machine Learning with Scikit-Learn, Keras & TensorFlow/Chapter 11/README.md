# ðŸ§  Bab 11: Melatih Jaringan Saraf Dalam  
ðŸ“ File: `11_training_deep_neural_networks.ipynb`

---

## ðŸŽ¯ Fokus Bab

Bab ini membahas **tantangan utama dalam pelatihan Deep Neural Networks (DNN)** dan menyediakan **teknik lanjutan** untuk mengatasinya. Topik-topik mencakup masalah optimasi, overfitting, inisialisasi bobot, dan penggunaan kembali model yang telah dilatih.

---

## âš ï¸ Tantangan Utama dalam Pelatihan DNN

- ðŸ“‰ **Vanishing / Exploding Gradients**  
  Gradien terlalu kecil atau terlalu besar, menghambat pelatihan lapisan terdalam.

- ðŸ§ª **Data Berlabel Tidak Cukup**  
  DNN membutuhkan data besar dan mahal untuk dilabeli secara manual.

- ðŸ¢ **Pelatihan Lambat**  
  Karena ukuran jaringan dan kompleksitas data.

- ðŸ§  **Overfitting**  
  Model kompleks mudah menyesuaikan data pelatihan terlalu baik, merugikan generalisasi.

---

## ðŸ§° Solusi untuk Masalah Gradien

- ðŸ“ **Inisialisasi Bobot**  
  - **Xavier (Glorot) Initialization**  
  - **He Initialization**

- ðŸ” **Fungsi Aktivasi yang Stabil**  
  - **ReLU**, **Leaky ReLU**, **ELU**, **SELU**

- âš–ï¸ **Normalisasi**  
  - **Batch Normalization**  
  - **Layer Normalization**

- âœ‚ï¸ **Gradient Clipping**  
  Membatasi ukuran gradien untuk mencegah exploding gradients.

---

## ðŸ” Transfer Learning & Pretraining

- ðŸ”„ **Transfer Learning**  
  Gunakan model yang telah dilatih sebelumnya (misal: ImageNet).

- ðŸ§© **Unsupervised Pretraining**  
  Pelajari representasi menggunakan autoencoder sebelum supervised learning.

- ðŸ§  **Auxiliary Tasks**  
  Gunakan tugas pembantu dengan banyak data sebagai pretraining.

---

## âš¡ Optimizer Canggih

- ðŸ’¡ Alternatif dari SGD biasa:
  - **Momentum**
  - **Nesterov**
  - **AdaGrad**
  - **RMSProp**
  - **Adam**, **AdaMax**, **Nadam**, **AdamW**

- ðŸ“‰ **Learning Rate Scheduling**  
  Sesuaikan learning rate selama pelatihan untuk hasil lebih stabil.

---

## ðŸ›¡ï¸ Regularisasi untuk Cegah Overfitting

- ðŸ§® **L1 / L2 Regularization**  
  Penalti terhadap besarnya bobot.

- ðŸ”„ **Dropout**  
  Nonaktifkan neuron acak saat pelatihan.

- ðŸŽ² **Monte Carlo Dropout**  
  Gunakan dropout juga saat inferensi untuk mengukur ketidakpastian.

- ðŸ“‰ **Max-Norm Regularization**  
  Batasi besarnya bobot maksimum per neuron.

---

## ðŸ“˜ Ringkasan

> Deep Learning bukan hanya soal membuat arsitektur besar, tapi juga menguasai seni **penyetelan, normalisasi, dan regularisasi**. Bab ini mengajarkan bahwa sukses dalam DL memerlukan pemahaman teknik canggih serta praktik terbaik dalam pelatihan model.

---
